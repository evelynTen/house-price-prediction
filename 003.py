# -*- coding: utf-8 -*-
"""WithPCALDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ubhcwXZ-0QjA1afvN51Z2cZTjKw2DHk

# Predict House price
## *Comparing 8 regression algorithms*

![house](https://i.imgur.com/HGsQXQS.png)

This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.

# Table of contents

[<h3>1. Data Analysis & Data Processing</h3>](#1)

[<h3>2. Model comparison</h3>](#2)

[<h3>3. Prediction metrics of the best model using the test set</h3>](#3)

[<h3>4. Visualization of the result</h3>](#4)

# Load the libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
# %matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_validate
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Lasso, LassoCV, ElasticNet, ElasticNetCV, Ridge, RidgeCV, LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from time import perf_counter
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import *
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler
import lightgbm as lgb
from mlxtend.regressor import StackingCVRegressor

from IPython.display import Markdown, display

def printmd(string):
    # Print with Markdowns    
    display(Markdown(string))

import warnings
warnings.filterwarnings(action='ignore')

"""# 1. Data Analysis & Data Processing<a class="anchor" id="1"></a><a class="anchor" id="1"></a>"""

import os
from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/My Drive"

os.chdir(path)
os.listdir(path)

import pandas as pd
df = pd.read_csv("/content/drive/My Drive/kc_house_data.csv")

# df = pd.read_csv('../input/housesalesprediction/kc_house_data.csv', index_col = 0)
df.head()

df.info()

df.describe()

df['price'].plot.box(by='price', color = '#ff8c8e')
plt.title('Display all the price\nInclusive outliers')
plt.show()

"""As we can see there are strong outliers. We'll filter them out and keep only the prices lower than 1.000.000."""

# df = df[df['price'] < 1000000]

fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))

df['price'].plot.hist(by='price',ax = axes[0], color = '#ff8c8e')
axes[0].set_title('price\'s histogram\n', fontsize = 15)

df['price'].plot.box(ax = axes[1])
axes[1].set_title('price\'s Boxplot\n', fontsize = 15)

sns.violinplot(ax = axes[2], y = 'price', data = df, color = '#ff8c8e')
axes[2].set_title('price\'s distribution\n(violinplot)', fontsize = 15)

plt.show()

printmd(f'### Number of rows in the dataset: {df.shape[0]}')

"""### Add a column to the DataFrame: age
The data are from the year 2014 and 2015. We'll take 2015 to simplify the calculation.The age is whether the number of years since the house was built or if it has been
renovated, the number of years since the renovation.
"""

# Add the column age
df['age'] = [2015 - x[0] if x[1]==0 else 2015 - x[1] for x in df[['yr_built','yr_renovated']].values]

# Display the result
df[['yr_built','yr_renovated','age']].head()

# Select some columns, which will be used in the regression model
cols =['price','bedrooms','bathrooms','sqft_living','sqft_lot','floors',
     'waterfront','view','condition','grade','sqft_above','sqft_basement',
     'yr_built','yr_renovated','lat','long','sqft_living15',
     'sqft_lot15','age']
df = df[cols]
df.head()

corrs = df.corr()
plt.figure(figsize=(16, 16))
sns.heatmap(corrs,annot = True)

# Visualization
sns.pairplot(df[cols], 
             kind='reg', 
             plot_kws={'line_kws':{'color':'black'}, 'scatter_kws': {'alpha': 0.005}},
             y_vars=['price'],
             x_vars=['sqft_living','grade','sqft_above','sqft_living15','bathrooms'],
             height = 4
            )
plt.show()

df.corr()['price'].sort_values(ascending = False)  #重点关注sqft_livin

# 异常值处理之前

df.plot(kind='scatter', x='long' , y='lat', alpha=0.4,
        c="price", cmap=plt.get_cmap("jet"), colorbar=True)
plt.xlabel('long')

def outliers_proc(data, col_name, scale=1):
    """
    用于清洗异常值，默认用 box_plot（scale=3）进行清洗
    :param data: 接收 pandas 数据格式
    :param col_name: pandas 列名
    :param scale: 尺度
    :return:
    """

    def box_plot_outliers(data_ser, box_scale):
        """
        利用箱线图去除异常值
        :param data_ser: 接收 pandas.Series 数据格式
        :param box_scale: 箱线图尺度，
        :return:
        """
        iqr = box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25))
        val_low = data_ser.quantile(0.25) - 1.5 * iqr
        val_up = data_ser.quantile(0.75) + 1.5 * iqr
        rule_low = (data_ser < val_low)
        rule_up = (data_ser > val_up)
        return (rule_low, rule_up), (val_low, val_up)

    data_n = data.copy()
    data_series = data_n[col_name]
    rule, value = box_plot_outliers(data_series, box_scale=scale)
    index = np.arange(data_series.shape[0])[rule[0] | rule[1]]
    print("Delete number is: {}".format(len(index)))
    data_n = data_n.drop(index)
    data_n.reset_index(drop=True, inplace=True)
    print("Now column number is: {}".format(data_n.shape[0]))
    index_low = np.arange(data_series.shape[0])[rule[0]]
    outliers = data_series.iloc[index_low]
    print("Description of data less than the lower bound is:")
    print(pd.Series(outliers).describe())
    index_up = np.arange(data_series.shape[0])[rule[1]]
    outliers = data_series.iloc[index_up]
    print("Description of data larger than the upper bound is:")
    print(pd.Series(outliers).describe())
    
    fig, ax = plt.subplots(1, 2, figsize=(5, 3))
    sns.boxplot(y=data[col_name], data=data, palette="Set1", ax=ax[0]).set(xlabel="before")
    sns.boxplot(y=data_n[col_name], data=data_n, palette="Set1", ax=ax[1]).set(xlabel="after")
    plt.tight_layout()
    return data_n

for i in df.columns.values.tolist():
    if str(df[i]) != 'int64':
        df[i] = df[i].apply(pd.to_numeric)
        df = outliers_proc(df,i)
    else:
        df = outliers_proc(df,i)

df.info()
df.describe()

# 异常值处理之后
df.plot(kind='scatter', x='long', y='lat', alpha=0.4,
        c="price", cmap=plt.get_cmap("jet"), colorbar=True).set_xlabel('long')

def preprocessing(df):
    df = df.copy()
       
    # Shuffle the data
    # df = df.sample(frac=1.0, random_state=0).reset_index(drop=True)
    
    X = df.drop('price', axis=1)
    y = df['price']
    
    X = pd.DataFrame(X, index=X.index, columns=X.columns)
    
    return X, y

# Preprocessing
X,y = preprocessing(df)

# Split into a training and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the datasets
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Display the result
print(X_train)
print(X_train.shape)

# PCA
# pca = PCA(n_components=0.98)
# pca.fit(X_train)
# X_train_dunction = pca.transform(X_train)
# X_test_dunction = pca.transform(X_test)

# print(X_train_dunction)
# print(X_train_dunction.shape)

# LDA
# lda = LinearDiscriminantAnalysis()
# lda.fit(X_train,y_train)
# X_train_lda = lda.transform(X_train)
# X_test_lda = lda.transform(X_test)

# print(X_train_lda)
# print(X_train_lda.shape)

#因为由前文可知，目标值明显右偏，故取对数，使其近似正态分布

sns.distplot(y_train, fit=norm)
print('Skewness of target:', y_train.skew())
print('kurtosis of target:', y_train.kurtosis())

y_train = np.log1p(y_train)
print('Skewness of target:', y_train.skew())
print('kurtosis of target:', y_train.kurtosis())
sns.distplot(y_train, fit=norm)

"""# 2. Model comparison<a class="anchor" id="2"></a>"""

#Lasso
lasso_alpha = [0.00005, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0]
lasso = make_pipeline(RobustScaler(), LassoCV(alphas=lasso_alpha, random_state=2))

#ElasticNet
#enet_beta = [0.1, 0.2, 0.5, 0.6, 0.8, 0.9]
#enet_alpha = [0.00005, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01]
#ENet = make_pipeline(RobustScaler(), ElasticNetCV(l1_ratio=enet_beta, alphas=enet_alpha, random_state=12))

#Ridge
#rid_alpha = [0.00005, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0]
#rid = make_pipeline(RobustScaler(), RidgeCV(alphas=rid_alpha))

#Gradient Boosting
gbr_params = {'loss': 'huber',
      'criterion': 'mse', 
      'learning_rate': 0.1,
      'n_estimators': 600, 
      'max_depth': 4,
      'subsample': 0.6,
      'min_samples_split': 20,
      'min_samples_leaf': 5,
      'max_features': 0.6,
      'random_state': 32,
      'alpha': 0.5}
gbr = GradientBoostingRegressor(**gbr_params)

#LightGBM
lgbr_params = {'learning_rate': 0.01,
      'n_estimators': 1850, 
      'max_depth': 8,
      'num_leaves': 50,
      'subsample': 0.6,
      'colsample_bytree': 0.6,
      'min_child_weight': 0.001,
      'min_child_samples': 21,
      'random_state': 42,
      'reg_alpha': 0,
      'reg_lambda': 0.05}
lgbr = lgb.LGBMRegressor(**lgbr_params)

#rfr = RandomForestRegressor(n_estimators=600,max_depth=16,min_samples_split=4,random_state=42)

lin = LinearRegression()

stack_model = StackingCVRegressor(regressors=(lasso, gbr, lgbr), meta_regressor=lasso, use_features_in_secondary=True)

models = {
    "LinearRegression":{"model":lin },
    "Lasso":{"model":lasso },
    # "Ridge":{"model":rid },
    # "DecisionTreeRegressor":{"model":DecisionTreeRegressor() },
    # "ElasticNet":{"model":ENet },
    # "RandomForestRegressor":{"model":rfr },
    # "MLPRegressor":{"model":MLPRegressor() },
    "LightGBM":{"model":lgbr },
    "GradientBoostingRegressor":{"model":gbr },
    "Stacking":{"model":stack_model }
    # "AdaBoostRegressor":{"model":AdaBoostRegressor() }
}

# Use the K-fold cross validation for each model
# to get the mean validation accuracy and the mean training time
k = 5
for name, m in models.items():
    # Cross validation of the model
    model = m['model']
    result = cross_validate(model, X_train,y_train, cv = k, scoring='neg_mean_squared_error')
    
    # Mean accuracy and mean training time
    result['test_score'] = result['test_score']
    mean_RMSE = [(-x)**0.5 for x in result['test_score']] # Root Mean Square Error
    mean_RMSE = sum(mean_RMSE)/len(mean_RMSE)
    # mean_RMSE = int(mean_RMSE)
    mean_fit_time = round( sum(result['fit_time']) / len(result['fit_time']), 4)
    
    # Add the result to the dictionary witht he models
    m['mean_RMSE'] = mean_RMSE
    m['Training time (sec)'] = mean_fit_time
    
    # Display the result
    print(f"{name:27} mean MSRE for {k}-fold CV: {mean_RMSE} - mean training time {mean_fit_time} sec")

# Create a DataFrame with the results
models_result = []

for name, v in models.items():
    lst = [name, v['mean_RMSE'],v['Training time (sec)']]
    models_result.append(lst)

df_results = pd.DataFrame(models_result, 
                          columns = ['model','RMSE','Training time (sec)'])
df_results.sort_values(by='RMSE', ascending=True, inplace=True)
df_results.reset_index(inplace=True,drop=True)
df_results

plt.figure(figsize = (15,5))
sns.barplot(x = 'model', y = 'RMSE', data = df_results)
plt.title(f'{k}-fold mean RMSE for each Model\nSmaller is better', fontsize = 15)
# plt.ylim(0.8,1.005)
plt.xlabel('Model', fontsize=15)
plt.ylabel('RMSE',fontsize=15)
plt.xticks(rotation=90, fontsize=12)
plt.show()

plt.figure(figsize = (15,5))
sns.barplot(x = 'model', y = 'Training time (sec)', data = df_results)
plt.title('Training time for each Model in sec\nSmaller is better', fontsize = 15)
plt.xticks(rotation=90, fontsize=12)
plt.xlabel('Model', fontsize=15)
plt.ylabel('Training time (sec)',fontsize=15)
plt.show()

"""# 3. Prediction metrics of the best model using the test set<a class="anchor" id="3"></a>"""

# Get the model with the highest mean validation accuracy
best_model = df_results.iloc[0]

# Fit the model
model = models[best_model[0]]['model']
model.fit(X_train,y_train)

# Predict the labels with the data set
pred = np.expm1(model.predict(X_test))

RMSE = mean_squared_error(y_test,pred)**0.5
RMSE = RMSE

# Display the results
printmd(f'### Best Model: {best_model[0]} with a RMSE of {RMSE} on the test set')
printmd(f'### Trained in: {best_model[2]} sec')

hos_pre = pd.DataFrame()
hos_pre['Predict'] = pred
hos_pre['Truth'] = y_test
hos_pre.plot(figsize = (18,8))
plt.ylim(0,1600000)
plt.xlim(0,200)

# Get the model with the highest mean validation accuracy
second_model = df_results.iloc[1]

# Fit the model
model = models[second_model[0]]['model']
model.fit(X_train,y_train)

# Predict the labels with the data set
pred = np.expm1(model.predict(X_test))

RMSE = mean_squared_error(y_test,pred)**0.5
RMSE = RMSE

# Display the results
printmd(f'### Best Model: {second_model[0]} with a RMSE of {RMSE} on the test set')
printmd(f'### Trained in: {second_model[2]} sec')

# Get the model with the highest mean validation accuracy
third_model = df_results.iloc[2]

# Fit the model
model = models[third_model[0]]['model']
model.fit(X_train,y_train)

# Predict the labels with the data set
pred = np.expm1(model.predict(X_test))

RMSE = mean_squared_error(y_test,pred)**0.5
RMSE = RMSE

# Display the results
printmd(f'### Best Model: {third_model[0]} with a RMSE of {RMSE} on the test set')
printmd(f'### Trained in: {third_model[2]} sec')

# Get the model with the highest mean validation accuracy
forth_model = df_results.iloc[3]

# Fit the model
model = models[forth_model[0]]['model']
model.fit(X_train,y_train)

# Predict the labels with the data set
pred = np.expm1(model.predict(X_test))

RMSE = mean_squared_error(y_test,pred)**0.5
RMSE = RMSE

# Display the results
printmd(f'### Best Model: {forth_model[0]} with a RMSE of {RMSE} on the test set')
printmd(f'### Trained in: {forth_model[2]} sec')

print(best_model)
print(df_results)

"""# 4. Visualization of the result<a class="anchor" id="4"></a>"""

# Concatenate the ratings of the test set
# with the predictions of those ratings
pred_s = pd.Series(pred)
y_test_s = y_test.reset_index(drop=True)

df_result = pd.concat([y_test_s,pred_s], axis = 1)
df_result.columns = ['Real Rating', 'Predicted Rating']
df_result.head(10)

df_result.plot.box()
plt.title('Boxplot Real Rating VS Predicted Rating', fontsize = 12)
plt.show()

df_result.plot.scatter(x='Real Rating', y='Predicted Rating', alpha = 0.1)
plt.title('Scatterplot Real Rating VS Predicted Rating', fontsize = 12)
plt.show()